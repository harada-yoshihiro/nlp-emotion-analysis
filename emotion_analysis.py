# -*- coding: utf-8 -*-
"""BERTãƒ¢ãƒ†ã‚™ãƒ«ã¨WRIMEãƒ†ã‚™ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ãŸæ–‡ç« ä¸­ã®æ‹æ„›æ„Ÿæƒ…æ¨å®š

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13BVwFT5laJt_R11KEgDcQyC4BIdKUngf

# BERTãƒ¢ãƒ†ã‚™ãƒ«ã¨WRIMEãƒ†ã‚™ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ãŸæ–‡ç« ä¸­ã®æ‹æ„›æ„Ÿæƒ…æ¨å®š

## ç ”ç©¶èª¬æ˜
BERT Ã— WRIME ã‚’ç”¨ã„ãŸè‡ªç„¶è¨€èªå‡¦ç†ã«ã‚ˆã‚‹æ„Ÿæƒ…æ§‹é€ ã®åˆ†æ

## æ¦‚è¦
æœ¬ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€WRIMEãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®8æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«ã‚’ç”¨ã„ã€
BERTã«ã‚ˆã‚Šæ–‡ç« ä¸­ã®æ„Ÿæƒ…åˆ†å¸ƒã‚’æ¨å®šã—ã€
è¤‡æ•°æ„Ÿæƒ…ã®é‡ã¿ä»˜ã‘ã«ã‚ˆã£ã¦ã€Œæ‹æ„›æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã€ã‚’å®šç¾©ãƒ»åˆ†æã™ã‚‹ã€‚

æ„Ÿæƒ…ã¨ã„ã†æ›–æ˜§ãªå¯¾è±¡ã‚’åˆ†è§£ãƒ»æ§‹é€ åŒ–ã™ã‚‹ã“ã¨ã§ã€
æ‹æ„›æ–‡ã¨éæ‹æ„›æ–‡ã®é•ã„ã‚’æ•°å€¤çš„ã«æ‰ãˆã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã‚‹ã€‚

è©³ç´°ãªèª¬æ˜ã¯æ¬¡ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ç¤ºã™ã€‚

## 1. ç ”ç©¶èƒŒæ™¯ãƒ»ç›®çš„
æ‹æ„›æ„Ÿæƒ…ã¯å–œã³ãƒ»æœŸå¾…ãƒ»ä¸å®‰ãƒ»æã‚Œãªã©è¤‡æ•°ã®æ„Ÿæƒ…ãŒè¤‡é›‘ã«çµ¡ã¿åˆã£ãŸã€å®šç¾©ã®æ›–æ˜§ãªæ„Ÿæƒ…ã§ã‚ã‚‹ã€‚
æ—¢å­˜ã®æ„Ÿæƒ…åˆ†æã§ã¯å˜ä¸€ãƒ©ãƒ™ãƒ«ã§æ‰±ã‚ã‚Œã‚‹ã“ã¨ãŒå¤šãã€æ‹æ„›ç‰¹æœ‰ã®æ„Ÿæƒ…æ§‹é€ ã‚’ååˆ†ã«è¡¨ç¾ã§ããªã„èª²é¡ŒãŒã‚ã‚‹ã€‚

æœ¬ç ”ç©¶ã§ã¯ã€WRIMEãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å«ã¾ã‚Œã‚‹8æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«ã‚’ç”¨ã„ã€
æ–‡ç« ã«å«ã¾ã‚Œã‚‹è¤‡æ•°æ„Ÿæƒ…ã®åˆ†å¸ƒã‚’ã‚‚ã¨ã«ã€Œæ‹æ„›æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã€ã‚’å®šç¾©ã™ã‚‹ã“ã¨ã§ã€
æ›–æ˜§ãªæ„Ÿæƒ…ã‚’æ§‹é€ åŒ–ãƒ»æ•°å€¤åŒ–ã§ãã‚‹ã‹ã‚’æ¤œè¨¼ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã™ã‚‹ã€‚

## 2. ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ¢ãƒ‡ãƒ«
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼šWRIMEï¼ˆæ—¥æœ¬èªæ„Ÿæƒ…åˆ†æãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰
- ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«ï¼šbert-base-japanese-whole-word-masking
- å®Ÿè£…ç’°å¢ƒï¼šPython / Google Colab

WRIMEã¯å„æ–‡ç« ã«å¯¾ã—ã¦8ç¨®é¡ã®æ„Ÿæƒ…ï¼ˆå–œã³ãƒ»æ‚²ã—ã¿ãƒ»æœŸå¾…ãƒ»é©šããƒ»æ€’ã‚Šãƒ»æã‚Œãƒ»å«Œæ‚ªãƒ»ä¿¡é ¼ï¼‰ãŒä»˜ä¸ã•ã‚Œã¦ãŠã‚Šã€è¤‡æ•°æ„Ÿæƒ…ã‚’åŒæ™‚ã«æ‰±ãˆã‚‹ç‚¹ãŒæœ¬ç ”ç©¶ã®ç›®çš„ã«é©ã—ã¦ã„ã‚‹ã€‚

## 3. æ‰‹æ³•æ¦‚è¦
1. BERTã‚’ç”¨ã„ã¦å…¥åŠ›æ–‡ç« ã”ã¨ã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã‚’æ¨å®š
2. æ‹æ„›æ„Ÿæƒ…ã«å¯„ä¸ã™ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã‚‹æ„Ÿæƒ…ã«é‡ã¿ã‚’ä»˜ä¸
3. å„æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã‚’åŠ é‡å¹³å‡ã—ã€æ‹æ„›æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã‚’ç®—å‡º
4. æ–‡ã®ç¨®é¡ï¼ˆå‘Šç™½æ–‡ãƒ»å¤±æ‹æ–‡ãƒ»é¢¨æ™¯æå†™æ–‡ãªã©ï¼‰ã”ã¨ã«æŒ™å‹•ã‚’æ¯”è¼ƒ

æ„Ÿæƒ…ã¨ã„ã†æŠ½è±¡çš„ãªå¯¾è±¡ã‚’ãã®ã¾ã¾æ‰±ã†ã®ã§ã¯ãªãã€
ã€Œã©ã®æ„Ÿæƒ…ãŒã©ã®ç¨‹åº¦å¯„ä¸ã—ã¦ã„ã‚‹ã‹ã€ã‚’åˆ†è§£ã—ã¦åˆ†æã™ã‚‹ã“ã¨ã‚’é‡è¦–ã—ãŸã€‚

## 4. å®Ÿè£…å†…å®¹
- ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†
- BERTã«ã‚ˆã‚‹æ„Ÿæƒ…æ¨å®š
- æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ–
- æ‹æ„›æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®ç®—å‡º
- æ–‡ä¾‹ã”ã¨ã®æ¨å®šçµæœã®å¯è¦–åŒ–

å„ã‚¹ãƒ†ãƒƒãƒ—ã§ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’ç¢ºèªã—ã€æƒ³å®šã¨ç•°ãªã‚‹çµæœãŒå‡ºãŸå ´åˆã¯åŸå› ã‚’æ¤œè¨ã—ãŸã€‚

## 5. å®Ÿé¨“çµæœ
å‘Šç™½æ–‡ã‚„æ‹æ„›ã«é–¢ã™ã‚‹æ„Ÿæƒ…è¡¨ç¾ã‚’å«ã‚€æ–‡ç« ã§ã¯ã€
å–œã³ãƒ»æœŸå¾…ãƒ»ä¿¡é ¼ãªã©ã®ã‚¹ã‚³ã‚¢ãŒé«˜ããªã‚Šã€æ‹æ„›æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã‚‚é«˜ãç®—å‡ºã•ã‚Œã‚‹å‚¾å‘ãŒç¢ºèªã§ããŸã€‚

ä¸€æ–¹ã§ã€æƒ…æ™¯æå†™ã‚’ä¸­å¿ƒã¨ã—ãŸæ–‡ç« ã«å¯¾ã—ã¦ã‚‚è² ã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ãŒåå¿œã™ã‚‹ã‚±ãƒ¼ã‚¹ãŒè¦‹ã‚‰ã‚Œã€æ‹æ„›æ„Ÿæƒ…ã¨ã¯ç„¡é–¢ä¿‚ãªæ–‡è„ˆã«å¯¾ã™ã‚‹èª¤åˆ¤å®šãŒèª²é¡Œã¨ã—ã¦æ®‹ã£ãŸã€‚

## 6. è€ƒå¯Ÿãƒ»èª²é¡Œ
- æ„Ÿæƒ…èªãŒæ˜ç¤ºã•ã‚Œãªã„æ–‡ç« ã«å¯¾ã—ã¦ã‚‚BERTãŒæ„Ÿæƒ…ã‚’æ¨å®šã—ã¦ã—ã¾ã†å‚¾å‘ãŒã‚ã‚‹
- ç‰¹ã«é¢¨æ™¯æå†™æ–‡ã§ã¯ã€æ‹æ„›æ„Ÿæƒ…ã¨ã¯ç›´æ¥é–¢ä¿‚ã—ãªã„è² ã®æ„Ÿæƒ…ãŒã‚¹ã‚³ã‚¢ã«å½±éŸ¿ã™ã‚‹
- æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®é‡ã¿ä»˜ã‘ãŒæ‰‹å‹•å®šç¾©ã§ã‚ã‚Šã€æœ€é©åŒ–ã®ä½™åœ°ãŒã‚ã‚‹

ä»Šå¾Œã¯ã€å¿ƒç†æå†™æ–‡ã¨æƒ…æ™¯æå†™æ–‡ã‚’äº‹å‰ã«åˆ†é¡ã™ã‚‹å‰å‡¦ç†ã‚„ã€é‡ã¿ã‚’å­¦ç¿’ã«ã‚ˆã£ã¦æœ€é©åŒ–ã™ã‚‹æ‰‹æ³•ã®å°å…¥ã‚’æ¤œè¨ã—ãŸã„ã€‚

## 7. ã¾ã¨ã‚
æœ¬ç ”ç©¶ã‚’é€šã—ã¦ã€æ›–æ˜§ã§æ„Ÿè¦šçš„ã«æ‰ãˆã‚‰ã‚ŒãŒã¡ãªæ„Ÿæƒ…ã‚‚ã€åˆ†è§£ãƒ»æ•´ç†ã™ã‚‹ã“ã¨ã§æŠ€è¡“çš„ã«æ‰±ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ãŸã€‚
æ„Ÿæƒ…ã®æ§‹é€ ã‚’åˆ†æã—ã€èª²é¡Œã‚’è¨€èªåŒ–ã—ãªãŒã‚‰æ”¹å–„ã‚’æ¤œè¨ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€ä»Šå¾Œã€è‡ªç„¶è¨€èªå‡¦ç†ã‚„ãƒ‡ãƒ¼ã‚¿åˆ†æã‚’ç”¨ã„ãŸèª²é¡Œè§£æ±ºã«æ´»ã‹ã›ã‚‹ã¨è€ƒãˆã¦ã„ã‚‹ã€‚

## ç’°å¢ƒæ§‹ç¯‰
"""

!pip install -U transformers

# huggingface transformer ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# - transformers : ä¸»ãŸã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆãƒ¢ãƒ‡ãƒ«ã‚„ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰)
# - datasets : HuggingFaceã§ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ‰±ã†ãŸã‚ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
# https://huggingface.co/docs/transformers/installation
! pip install transformers datasets

# æ±åŒ—å¤§å­¦ã®æ—¥æœ¬èªç”¨BERTä½¿ç”¨ã«å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
! pip install fugashi ipadic

import transformers
print(transformers.__version__)

import numpy as np
import pandas as pd

# Hugging Face (Transformers) é–¢é€£ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import TrainingArguments, Trainer
from datasets import Dataset

"""### Matplotlibã§æ—¥æœ¬èªã‚’ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹"""

# [å‰æº–å‚™] Matplotlib ã§æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
# cf. https://blog.3qe.us/entry/2018/08/16/121457
!apt-get -y install fonts-ipafont-gothic
!rm /root/.cache/matplotlib/fontlist-v310.json

# NOTE ã“ã“ã§ã€ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•

!apt-get -y install fonts-ipafont-gothic
!pip install japanize-matplotlib
import japanize_matplotlib
import matplotlib.pyplot as plt

plt.rcParams['font.family'] = 'IPAPGothic'
japanize_matplotlib.japanize()

import matplotlib.pyplot as plt
import seaborn as sns

# å‹•ä½œç¢ºèª
plt.figure(figsize=(5,1))
plt.title('æ—¥æœ¬èªã‚’è¡¨ç¤ºã§ãã‚‹ã‹ãƒ†ã‚¹ãƒˆ')

"""## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™

### WRIMEãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
"""

# GitHubã‚ˆã‚ŠWRIMEãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
#
# WRIME dataset : https://github.com/ids-cv/wrime
# ä»Šå›ä½¿ç”¨ã™ã‚‹ã®ã¯ ver1 ï¼ˆæ„Ÿæƒ…æ¥µæ€§ãŒä»˜ä¸ã•ã‚Œã¦ã„ãªã„ç‰ˆï¼‰
! wget https://github.com/ids-cv/wrime/raw/master/wrime-ver1.tsv

# pandas.DataFrameã¨ã—ã¦èª­ã¿è¾¼ã‚€
df_wrime = pd.read_table('wrime-ver1.tsv')
df_wrime.head(2)

"""### å‰å‡¦ç†

__å‰æâ‘ ï¼šWRIMEãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ©ãƒ™ãƒ«ã¯ã€å„æ„Ÿæƒ…ã®å¼·åº¦__
- å„æ„Ÿæƒ…ï¼ˆä¾‹ãˆã°ã€Joy=å–œï¼‰ã®å¼·åº¦ã‚’ã€0ã€œ3ã®ï¼”æ®µéšã§ãƒ©ãƒ™ãƒ«ä»˜ã‘ã—ã¦ã„ã‚‹
- ï¼˜ã¤ã®æ„Ÿæƒ…å…¨ã¦ã«ãŠã„ã¦ã€æœ€é »ãƒ©ãƒ™ãƒ«ã¯ã€Œ0ã€


__å‰æâ‘¡ï¼šWRIMEãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ã€è¤‡æ•°ç¨®é¡ã®ãƒ©ãƒ™ãƒ«ãŒä»˜ä¸ã•ã‚Œã¦ã„ã‚‹__
1. Writer_*
    - æ–‡ç« ã® __æ›¸ãæ‰‹__ ãŒè‡ªèº«ã§ä»˜ä¸ã—ãŸãƒ©ãƒ™ãƒ«ã€‚ã€Œä¸»è¦³æ„Ÿæƒ…ã€ã€‚
2. Reader{1,2,3}_* :
    - æ–‡ç« ã® __èª­ã¿æ‰‹__ ãŒä»˜ä¸ã—ãŸãƒ©ãƒ™ãƒ«ã€‚ã€Œå®¢è¦³æ„Ÿæƒ…ã€ã€‚ï¼“ååˆ†ã€‚
3. Avg.Readers_*
    - ï¼“ååˆ†ã®å®¢è¦³æ„Ÿæƒ…ã®å¹³å‡å€¤ã€‚

å•é¡Œè¨­å®š
1. ï¼˜ã¤ã®æ„Ÿæƒ…ã®ã€åˆ†é¡ã‚¿ã‚¹ã‚¯ã¨ã—ã¦æ‰±ã†
    - ç›¸å¯¾çš„ã«ã©ã®æ„Ÿæƒ…ãŒå¼·ã„ã‹ã‚’æ¨å®šã™ã‚‹
    - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æœ¬æ¥ã®ç”¨é€”ã¨ã—ã¦ã¯æ„Ÿæƒ…å¼·åº¦ã‚’æ¨å®šã™ã‚‹ã‚¿ã‚¹ã‚¯ã€‚ã—ã‹ã—ãªãŒã‚‰ã€æ„Ÿæƒ…å¼·åº¦=0ã®ã‚µãƒ³ãƒ—ãƒ«ãŒå¤šãã€ã‚„ã‚„æ‰±ã„ãŒé›£ã—ã„ãŸã‚ã€ä»Šå›ã¯ç°¡ç´ åŒ–ã—ã¦æ‰±ã†ã€‚
2. å®¢è¦³æ„Ÿæƒ…ã®å¹³å‡å€¤ã‚’ä½¿ç”¨ã™ã‚‹
    - è«–æ–‡ã«ãŠã„ã¦ã€ä¸»è¦³æ„Ÿæƒ…ã¨å®¢è¦³æ„Ÿæƒ…ã¯ç•°ãªã‚‹ã“ã¨ãŒæŒ‡æ‘˜ã•ã‚Œã¦ã„ã‚‹
    - ä¸»è¦³æ„Ÿæƒ…ã¯ã€æ›¸ãæ‰‹ã®æ€§æ ¼ã‚„è¡¨ç¾æ–¹æ³•ã«ä¾å­˜ã™ã‚‹éƒ¨åˆ†ãŒã‚ã‚‹ã€‚ãã®ãŸã‚ã€å®¢è¦³æ„Ÿæƒ…ã€ã‹ã¤ã€ãã®å¹³å‡å€¤ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€æ¨å®šçµæœã®ç´å¾—æ„ŸãŒé«˜ããªã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã‚‹
"""

# Plutchikã®8ã¤ã®åŸºæœ¬æ„Ÿæƒ…
emotion_names = ['Joy', 'Sadness', 'Anticipation', 'Surprise', 'Anger', 'Fear', 'Disgust', 'Trust']
emotion_names_jp = ['å–œã³', 'æ‚²ã—ã¿', 'æœŸå¾…', 'é©šã', 'æ€’ã‚Š', 'æã‚Œ', 'å«Œæ‚ª', 'ä¿¡é ¼']  # æ—¥æœ¬èªç‰ˆ
num_labels = len(emotion_names)

# readers_emotion_intensities åˆ—ã‚’ç”Ÿæˆã™ã‚‹
# "Avg. Readers_*" ã®å€¤ã‚’liståŒ–ã—ãŸã‚‚ã®
df_wrime['readers_emotion_intensities'] = df_wrime.apply(lambda x: [x['Avg. Readers_' + name] for name in emotion_names], axis=1)

# æ„Ÿæƒ…å¼·åº¦ãŒä½ã„ã‚µãƒ³ãƒ—ãƒ«ã¯é™¤å¤–ã™ã‚‹
# (readers_emotion_intensities ã® max ãŒ1ä»¥ä¸Šã®ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹)
is_target = df_wrime['readers_emotion_intensities'].map(lambda x: max(x) >= 1)
df_wrime_target = df_wrime[is_target]

# train / test ã«åˆ†å‰²ã™ã‚‹
df_groups = df_wrime_target.groupby('Train/Dev/Test')
df_train = df_groups.get_group('train')
df_test = pd.concat([df_groups.get_group('dev'), df_groups.get_group('test')])
print('train :', len(df_train))
print('test :', len(df_test))

"""## ãƒ¢ãƒ‡ãƒ«(BERT)ã‚’è¨“ç·´ã™ã‚‹

### Tokenizerã§å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
"""

!pip install unidic-lite fugashi[unidic-lite]

import unidic_lite
import fugashi

tagger = fugashi.Tagger()
print(tagger.parse("ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆã§ã™ã€‚"))

# ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ã¦ã€Tokenizerã‚’èª­ã¿è¾¼ã‚€
checkpoint = 'cl-tohoku/bert-base-japanese-whole-word-masking'
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# å‰å‡¦ç†é–¢æ•°: tokenize_function
# æ„Ÿæƒ…å¼·åº¦ã®æ­£è¦åŒ–ï¼ˆç·å’Œ=1ï¼‰ã‚‚åŒæ™‚ã«å®Ÿæ–½ã™ã‚‹
def tokenize_function(batch):
    tokenized_batch = tokenizer(batch['Sentence'], truncation=True, padding='max_length')
    tokenized_batch['labels'] = [x / np.sum(x) for x in batch['readers_emotion_intensities']]  # ç·å’Œ=1ã«æ­£è¦åŒ–
    return tokenized_batch

# Transformersç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå½¢å¼ã«å¤‰æ›
# pandas.DataFrame -> datasets.Dataset
target_columns = ['Sentence', 'readers_emotion_intensities']
train_dataset = Dataset.from_pandas(df_train[target_columns])
test_dataset = Dataset.from_pandas(df_test[target_columns])

# å‰å‡¦ç†ï¼ˆtokenize_functionï¼‰ ã‚’é©ç”¨
train_tokenized_dataset = train_dataset.map(tokenize_function, batched=True)
test_tokenized_dataset = test_dataset.map(tokenize_function, batched=True)

"""### è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""

# åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ AutoModelForSequenceClassification ã‚’ä½¿ç”¨ã™ã‚‹
# checkpoint ã¨ num_labelsï¼ˆã‚¯ãƒ©ã‚¹æ•°ï¼‰ ã‚’æŒ‡å®šã™ã‚‹. ä»Šå›ã¯ã€ã„ãšã‚Œã‚‚ä¸Šã§å®šç¾©æ¸ˆã¿
# - checkpoint = 'cl-tohoku/bert-base-japanese-whole-word-masking'
# - num_labels = 8
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)

"""### è¨“ç·´ã‚’å®Ÿè¡Œ"""

# accuracyã«ã‚ˆã‚‹è©•ä¾¡ã¯ä»Šå›ã®ç ”ç©¶ç›®çš„ã¨åˆè‡´ã—ãªã„ãŸã‚ä½¿ç”¨ã—ãªã„
# metric = load_metric("accuracy")
#
# def compute_metrics(eval_pred):
#     logits, labels = eval_pred
#     predictions = np.argmax(logits, axis=-1)
#     label_ids = np.argmax(labels, axis=-1)
#     return metric.compute(predictions=predictions, references=label_ids)

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="test_trainer",
    per_device_train_batch_size=8,
    num_train_epochs=1,
    report_to=[],
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized_dataset,
    eval_dataset=test_tokenized_dataset,
)

trainer.train()

model.save_pretrained("my_model")
tokenizer.save_pretrained("my_model")

!zip -r my_model.zip my_model
from google.colab import files
files.download("my_model.zip")

"""## è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã§æ¨è«–ã™ã‚‹"""

# https://www.delftstack.com/ja/howto/numpy/numpy-softmax/
def np_softmax(x):
    f_x = np.exp(x) / np.sum(np.exp(x))
    return f_x

def analyze_emotion(text, show_fig=False, ret_prob=False):
    # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã‹
    model.eval()

    # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿å¤‰æ› + æ¨è«–
    tokens = tokenizer(text, truncation=True, return_tensors="pt")
    tokens.to(model.device)
    preds = model(**tokens)
    prob = np_softmax(preds.logits.cpu().detach().numpy()[0])
    out_dict = {n: p for n, p in zip(emotion_names_jp, prob)}

    # æ£’ã‚°ãƒ©ãƒ•ã‚’æç”»
    if show_fig:
        plt.figure(figsize=(8, 3))
        df = pd.DataFrame(out_dict.items(), columns=['name', 'prob'])
        sns.barplot(x='name', y='prob', data=df)
        plt.title('å…¥åŠ›æ–‡ : ' + text, fontsize=15)

    if ret_prob:
        return out_dict

# å‹•ä½œç¢ºèª
analyze_emotion('ä»Šæ—¥ã‹ã‚‰é•·æœŸä¼‘æš‡ã ããƒ¼ãƒ¼ãƒ¼ï¼ï¼ï¼', show_fig=True)

analyze_emotion('ã“ã®æ›¸é¡ã«ã¯ã‚³ãƒ¼ãƒ’ãƒ¼ã‹ã‹ã£ã¦ãªãã¦è‰¯ã‹ã£ãŸâ€¦ã€‚ä¸å¹¸ä¸­ã®å¹¸ã„ã ã€‚', show_fig=True)

analyze_emotion('ãªã‚“ã§è‡ªåˆ†ã ã‘ã“ã‚“ãªç›®ã«é­ã†ã‚“ã â€¦â€¦', show_fig=True)

analyze_emotion('å›ãªã‚‰ãã£ã¨ã‚„ã£ã¦ãã‚Œã‚‹ã¨æ€ã£ã¦ã„ãŸã‚ˆï¼', show_fig=True)

analyze_emotion('ãˆã€ä»Šæ—¥ã£ã¦ä¼‘æ ¡ã ã£ãŸã®ï¼Ÿ', show_fig=True)

analyze_emotion('æ˜æ—¥ã®ãƒ—ãƒ¬ã‚¼ãƒ³ã†ã¾ãã§ãã‚‹ã‹ãªãâ€¦', show_fig=True)

analyze_emotion('ã‚ããƒ¼ã€ã‚¤ãƒ©ã‚¤ãƒ©ã™ã‚‹ã£ï¼ï¼', show_fig=True)

import re

def split_text_strict(text):
    # æ”¹è¡Œã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›
    text = text.replace('\n', ' ').replace('\r', ' ')

    # å…¨è§’ãƒ»åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’ã™ã¹ã¦å‰Šé™¤
    text = re.sub(r'[ ã€€]+', '', text)

    # å¥ç‚¹ã€æ„Ÿå˜†ç¬¦ã€ç–‘å•ç¬¦ã§æ–‡ã‚’åŒºåˆ‡ã‚‹
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ])', text)

    # ç©ºæ–‡å­—ã‚’é™¤å»
    return [s for s in sentences if s]

import numpy as np
import pandas as pd
import seaborn as sns

def analyze_cumulative_emotion(text):
    # æ–‡ç« ã‚’æ–‡ã”ã¨ã«åˆ†å‰²
    sentences = split_text_strict(text)

    # æ„Ÿæƒ…åï¼ˆãƒ¢ãƒ‡ãƒ«ã¨åˆã‚ã›ã¦ãŠãï¼‰
    emotion_names = emotion_names_jp

    # æ„Ÿæƒ…ã”ã¨ã®ç´¯ç©ã‚¹ã‚³ã‚¢åˆæœŸåŒ–
    cumulative_scores = {emotion: 0.0 for emotion in emotion_names}

    # å„æ–‡ã«å¯¾ã—ã¦æ„Ÿæƒ…æ¨å®šã‚’å®Ÿè¡Œã—ã¦åŠ ç®—
    for sentence in sentences:
        scores = analyze_emotion(sentence, ret_prob=True)
        for emotion in emotion_names:
            cumulative_scores[emotion] += scores.get(emotion, 0.0)

    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ åŒ–
    df = pd.DataFrame(cumulative_scores.items(), columns=["æ„Ÿæƒ…", "ç´¯ç©ã‚¹ã‚³ã‚¢"])

    # ã‚°ãƒ©ãƒ•æç”»
    plt.figure(figsize=(8, 4))
    sns.barplot(x="æ„Ÿæƒ…", y="ç´¯ç©ã‚¹ã‚³ã‚¢", data=df)
    plt.title("ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã«ãŠã‘ã‚‹æ„Ÿæƒ…ã®ç´¯ç©", fontsize=16)
    plt.ylabel("ã‚¹ã‚³ã‚¢ã®åˆè¨ˆ")
    plt.xlabel("æ„Ÿæƒ…")
    plt.tight_layout()
    plt.show()

    return df

text = """
ä»Šæ—¥ã¯æœã‹ã‚‰ã„ã„å¤©æ°—ã§ã€æ°—åˆ†ãŒã¨ã¦ã‚‚ã‚ˆã‹ã£ãŸã€‚
ã§ã‚‚å¸°ã‚Šé“ã§é›¨ã«é™ã‚‰ã‚Œã¦ã³ã—ã‚‡æ¿¡ã‚Œã«ãªã£ã¦ã—ã¾ã„ã€ã¡ã‚‡ã£ã¨ã‚¤ãƒ©ã‚¤ãƒ©ã—ãŸã€‚
æ˜æ—¥ã¯æ™´ã‚Œã‚‹ã¨ã„ã„ãªï¼
"""

analyze_cumulative_emotion(text)

text = """ä»Šæ—¥ã¯æœã‹ã‚‰é›¨ãŒé™ã£ã¦ã„ã¦ã€å°‘ã—è‚Œå¯’ãæ„Ÿã˜ãŸã€‚
ã‚³ãƒ¼ãƒ’ãƒ¼ã‚’é£²ã¿ãªãŒã‚‰ã€çª“ã®å¤–ã‚’ã¼ã‚“ã‚„ã‚Šã¨çœºã‚ã¦ã„ã‚‹ã¨ã€
é™ã‹ãªæ™‚é–“ãŒå°‘ã—ã ã‘å¿ƒã‚’è½ã¡ç€ã‹ã›ã¦ãã‚ŒãŸã€‚"""

analyze_cumulative_emotion(text)

"""## æ‹æ„›æ„Ÿæƒ…åˆ†æ"""

# ==============================
# ğŸ’¡ é–¢æ•°å®šç¾©ï¼ˆæ„Ÿæƒ…åˆ†æ + å¯è¦–åŒ–ï¼‰
# ==============================

import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# --- æ–‡åˆ†å‰²é–¢æ•° ---
def split_text_strict(text):
    """
    å¥ç‚¹ï¼ˆã€‚ï¼ï¼Ÿï¼‰ã¾ãŸã¯é–‰ã˜é‰¤æ‹¬å¼§ï¼ˆã€ï¼‰ã§æ–‡ã‚’åŒºåˆ‡ã‚‹ã€‚
    ç©ºç™½ã‚„æ”¹è¡Œã‚‚æ•´ãˆã€ç©ºæ–‡ã¯é™¤å»ã€‚
    """
    text = text.replace('\n', ' ').replace('\r', ' ')
    text = re.sub(r'[ ã€€]+', ' ', text)

    # æ–‡æœ«ã®å¥ç‚¹ã‹é–‰ã˜é‰¤æ‹¬å¼§ã§åŒºåˆ‡ã‚‹æ­£è¦è¡¨ç¾
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿã€])', text)

    # ç©ºæ–‡ãƒ»ç©ºç™½ã®ã¿ã®è¦ç´ ã‚’é™¤å»ã—ã€ãƒˆãƒªãƒ ã—ã¦è¿”ã™
    return [s.strip() for s in sentences if s.strip()]

# --- æ–‡ç•ªå·è¡¨ç¤ºé–¢æ•° ---
def print_numbered_sentences(text):
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’å¥ç‚¹ã¾ãŸã¯é–‰ã˜é‰¤æ‹¬å¼§ã§åŒºåˆ‡ã‚Šã€
    æ–‡ç•ªå·ã‚’ã¤ã‘ã¦1æ–‡ãšã¤ãƒ—ãƒªãƒ³ãƒˆã™ã‚‹é–¢æ•°ã€‚
    """
    sentences = split_text_strict(text)

    for i, sentence in enumerate(sentences, 1):
        print(f"{i}: {sentence}")

# --- softmaxé–¢æ•° ---
def np_softmax(x):
    f_x = np.exp(x) / np.sum(np.exp(x))
    return f_x

# --- æ„Ÿæƒ…æ¨å®šé–¢æ•° ---
def analyze_emotion(text, show_fig=False, ret_prob=False):
    model.eval()
    tokens = tokenizer(text, truncation=True, return_tensors="pt")
    tokens.to(model.device)
    preds = model(**tokens)
    prob = np_softmax(preds.logits.cpu().detach().numpy()[0])
    out_dict = {n: p for n, p in zip(emotion_names_jp, prob)}

    if show_fig:
        plt.figure(figsize=(8, 3))
        df = pd.DataFrame(out_dict.items(), columns=['name', 'prob'])
        sns.barplot(x='name', y='prob', data=df)
        plt.title('å…¥åŠ›æ–‡ : ' + text, fontsize=15)

    if ret_prob:
        return out_dict

# --- æ„Ÿæƒ…ã®é †ç•ªã¨ãƒãƒ¬ãƒ³ã‚¹å®šç¾© ---
emotion_names_jp = ['å–œã³', 'æ‚²ã—ã¿', 'æœŸå¾…', 'é©šã', 'æ€’ã‚Š', 'æã‚Œ', 'å«Œæ‚ª', 'ä¿¡é ¼']
VALENCE = {
    'å–œã³': +1,
    'æ‚²ã—ã¿': -1,
    'æœŸå¾…': +0.5,
    'é©šã':  0.0,
    'æ€’ã‚Š': -1,
    'æã‚Œ': -0.5,
    'å«Œæ‚ª': -1,
    'ä¿¡é ¼': +1
}

# --- æ„Ÿæƒ…æ›²ç·šã¨ç´¯ç©ã‚¹ã‚³ã‚¢ã®è¨ˆç®— ---
def analyze_full_emotion_curve(text):
    sentences = split_text_strict(text)
    transitions = []
    cumulative_valence = []
    total_valence = 0.0

    for sentence in sentences:
        scores = analyze_emotion(sentence, ret_prob=True)
        transitions.append(scores)
        val = sum(VALENCE[k] * scores[k] for k in emotion_names_jp)
        total_valence += val
        cumulative_valence.append(total_valence)

    return transitions, cumulative_valence, sentences

# --- ã‚°ãƒ©ãƒ•æç”» ---
def plot_full_emotion_curve(transitions, cumulative_valence, sentences):
    df = pd.DataFrame(transitions)
    df["æ–‡ç•ªå·"] = range(1, len(df)+1)
    df = df.set_index("æ–‡ç•ªå·")

    plt.figure(figsize=(14, 6))
    for emo in df.columns:
        plt.plot(df.index, df[emo], marker='o', label=emo)
    plt.title("æ„Ÿæƒ…æ›²ç·šï¼ˆ8æ„Ÿæƒ…ï¼‰", fontsize=14)
    plt.xlabel("æ–‡ç•ªå·")
    plt.ylabel("æ„Ÿæƒ…ã‚¹ã‚³ã‚¢")
    plt.legend(ncol=4)
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    plt.figure(figsize=(14, 4))
    plt.plot(range(1, len(cumulative_valence)+1), cumulative_valence, color='crimson', marker='o')
    plt.title("ç´¯ç©æ‹æ„›æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®æ¨ç§»", fontsize=14)
    plt.xlabel("æ–‡ç•ªå·")
    plt.ylabel("ã‚¹ã‚³ã‚¢ï¼ˆæ­£è² ï¼‰")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# ---ç´¯ç©æ‹æ„›æ„Ÿæƒ…ãŒæ¸›å°‘ã—ãŸæ–‡ç« ãƒã‚§ãƒƒã‚¯---
def analyze_and_print_decreasing_valence(text):
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’æ–‡å˜ä½ã«æ„Ÿæƒ…åˆ†æã—ã€ç´¯ç©æ‹æ„›æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ãŒæ¸›å°‘ã—ãŸæ–‡ã®
    ç•ªå·ãƒ»å†…å®¹ãƒ»ã‚¹ã‚³ã‚¢é·ç§»ãƒ»æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤ºã™ã‚‹ã€‚
    """
    # æ„Ÿæƒ…åˆ†æã¨ã‚¹ã‚³ã‚¢ç®—å‡º
    transitions, cumulative_valence, sentences = analyze_full_emotion_curve(text)

    print("â–¼ ç´¯ç©æ‹æ„›æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ãŒæ¸›å°‘ã—ãŸæ–‡:\n")
    found = False

    for i in range(1, len(cumulative_valence)):
        if cumulative_valence[i] < cumulative_valence[i-1]:
            found = True
            print(f"[æ–‡{i+1}] {sentences[i]}")
            print(f"  ç´¯ç©ã‚¹ã‚³ã‚¢: {cumulative_valence[i-1]:.3f} â†’ {cumulative_valence[i]:.3f}")
            print()

    if not found:
        print("ï¼ˆæ¸›å°‘ã—ãŸç®‡æ‰€ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼‰")

# --- æ‹æ„›å°èª¬é¢¨ã®ãƒ†ã‚­ã‚¹ãƒˆ---
text = """
å½¼å¥³ã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒæ¥ã‚‹ãŸã³ã«ã€å¿ƒãŒè¸Šã£ãŸã€‚
ã—ã‹ã—ã‚ã‚‹æ—¥ã€è¿”äº‹ãŒã±ã£ãŸã‚Šã¨æ¥ãªããªã£ãŸã€‚
ä¸å®‰ã¨æã‚ŒãŒèƒ¸ã‚’å ã‚ã€æ€’ã‚Šã¨æ‚²ã—ã¿ãŒäº¤éŒ¯ã—ãŸã€‚
ãã‚Œã§ã‚‚ã€æœŸå¾…ã ã‘ã¯æ¨ã¦ã‚‰ã‚Œãšã€å½¼å¥³ã‚’å¾…ã¡ç¶šã‘ãŸã€‚
æ•°æ—¥å¾Œã€å½¼å¥³ã‹ã‚‰ã®çªç„¶ã®é›»è©±ã«ã€å–œã³ãŒçˆ†ç™ºã—ãŸã€‚
å„ªã—ã„å£°ã‚’èã„ãŸç¬é–“ã€ä¿¡é ¼ã¨æ„›æƒ…ãŒå†ã³è˜‡ã£ãŸã€‚
"""

# --- å®Ÿè¡Œ ---
transitions, cumulative_valence, sentences = analyze_full_emotion_curve(text)
plot_full_emotion_curve(transitions, cumulative_valence, sentences)

text1 = """æ ¡èˆã®å±‹ä¸Šã«ã€å¤•æš®ã‚Œã®å…‰ãŒé™ã‹ã«å·®ã—è¾¼ã‚“ã§ã„ãŸã€‚
å†·ãˆãŸé¢¨ãŒé ¬ã‚’æ’«ã§ã€äºŒäººã®å½±ã‚’é•·ãä¼¸ã°ã™ã€‚
å½¼ã¯éš£ã«ç«‹ã¤å½¼å¥³ã‚’è¦‹ãŸã€‚æ²ˆé»™ãŒãã®å ´ã‚’æ”¯é…ã—ã¦ã„ãŸãŒã€äº’ã„ã®é¼“å‹•ã¯ç¢ºã‹ã«èã“ãˆã¦ã„ãŸã€‚
ã‚†ã£ãã‚Šã¨ã€å½¼ã¯å£ã‚’é–‹ã„ãŸã€‚
ã€Œè©±ãŒã‚ã‚‹ã€
å½¼å¥³ã¯è»½ãã†ãªãšã„ãŸã€‚ç›®ã¯ã˜ã£ã¨å½¼ã‚’æ‰ãˆã¦ã„ã‚‹ã€‚
å½¼ã¯æ¯ã‚’æ•´ãˆã€è¨€è‘‰ã‚’é¸ã‚“ã§ã‹ã‚‰ç¶šã‘ãŸã€‚
ã€Œé•·ã„ã‚ã„ã ã€ãšã£ã¨è€ƒãˆã¦ã„ãŸã€‚å›ã¨éã”ã™æ™‚é–“ãŒã€è‡ªåˆ†ã«ã¨ã£ã¦ã©ã‚Œã»ã©å¤§åˆ‡ã‹ã‚’ã€
ãã®è¨€è‘‰ã¯ã€é£¾ã‚Šæ°—ã®ãªã„çœŸæ‘¯ãªã‚‚ã®ã ã£ãŸã€‚
å½¼å¥³ã®ç³ã«ã€å¾®ã‹ãªå…‰ãŒå®¿ã£ãŸã€‚
ã€Œç§ã‚‚åŒã˜æ°—æŒã¡ã ã£ãŸã€
å½¼å¥³ã®å£°ã¯é™ã‹ã§ã€ç¢ºã‹ãªéŸ¿ãã‚’æŒã£ã¦ã„ãŸã€‚
å½¼ã¯ã‚†ã£ãã‚Šã¨å½¼å¥³ã®æ‰‹ã‚’å–ã‚Šã€æš–ã‹ã•ã‚’æ„Ÿã˜ãŸã€‚
é¢¨ãŒã•ã‚‰ã«å¼·ãå¹ãã€å¤•é™½ãŒæ²ˆã¿ã‹ã‘ã‚‹ç©ºã‚’æœ±ã«æŸ“ã‚ã¦ã„ãŸã€‚
äºŒäººã¯è¨€è‘‰ã‚’äº¤ã‚ã™ã“ã¨ãªãã€ãŸã è¦‹ã¤ã‚åˆã£ãŸã€‚
ãã®ç¬é–“ã‹ã‚‰ã€å½¼ã‚‰ã¯æ‹äººã¨ãªã£ãŸã®ã ã€‚"""

text2 = """æ ¡èˆã®å±‹ä¸Šã«ã€å¤•æš®ã‚Œã®å…‰ãŒé™ã‹ã«å·®ã—è¾¼ã‚“ã§ã„ãŸã€‚
å†·ãˆãŸé¢¨ãŒé ¬ã‚’æ’«ã§ã€äºŒäººã®å½±ã‚’é•·ãä¼¸ã°ã™ã€‚
å½¼ã¯é™ã‹ã«å½¼å¥³ã®å‰ã«ç«‹ã¡ã€æ±ºå¿ƒã‚’å›ºã‚ãŸå£°ã§åˆ‡ã‚Šå‡ºã—ãŸã€‚
ã€Œå›ã«ä¼ãˆãŸã„ã“ã¨ãŒã‚ã‚‹ã€
å½¼å¥³ã¯å½¼ã®è¨€è‘‰ã‚’å¾…ã¡ã€ã‚ãšã‹ã«ã†ãªãšã„ãŸã€‚
å½¼ã¯ç¶šã‘ãŸã€‚
ã€Œé•·ã„é–“ã€è€ƒãˆã¦ã„ãŸã€‚å›ã®ã“ã¨ã‚’æƒ³ã†æ°—æŒã¡ã¯æœ¬ç‰©ã ã€
ã ãŒã€å½¼å¥³ã®ç³ã¯ã©ã“ã‹é ãã‚’è¦‹ã¦ã„ãŸã€‚
ã—ã°ã‚‰ãã®æ²ˆé»™ã®å¾Œã€å½¼å¥³ã¯é™ã‹ã«å£ã‚’é–‹ã„ãŸã€‚
ã€Œã‚ã‚ŠãŒã¨ã†ã€‚ã‚ãªãŸã®æ°—æŒã¡ã¯å¬‰ã—ã„ã‘ã‚Œã©ã€ç§ã«ã¯ã¾ã è‡ªåˆ†ã®æ°—æŒã¡ãŒã¯ã£ãã‚Šã—ãªã„ã®ã€
ã€Œä»Šã¯æ‹äººã«ãªã‚‹è¦šæ‚ŸãŒã§ãã¦ã„ãªã„ã€
å½¼ã¯ãã®è¨€è‘‰ã‚’å—ã‘æ­¢ã‚ã€ã‹ã™ã‹ãªè‹¦ã¿ã‚’èƒ¸ã«åˆ»ã‚“ã ã€‚
å¤•é™½ãŒæ²ˆã¿ã€æ ¡èˆã®å½±ãŒé•·ãä¼¸ã³ã‚‹ã€‚
äºŒäººã¯è¨€è‘‰ã‚’äº¤ã‚ã•ãšã€ãã‚Œãã‚Œã®æ€ã„ã‚’èƒ¸ã«å±‹ä¸Šã‚’å¾Œã«ã—ãŸã€‚"""

print_numbered_sentences(text1)

# --- å®Ÿè¡Œ ---
transitions, cumulative_valence, sentences = analyze_full_emotion_curve(text1)
plot_full_emotion_curve(transitions, cumulative_valence, sentences)

analyze_and_print_decreasing_valence(text1)

print_numbered_sentences(text2)

# --- å®Ÿè¡Œ ---
transitions, cumulative_valence, sentences = analyze_full_emotion_curve(text2)
plot_full_emotion_curve(transitions, cumulative_valence, sentences)

analyze_and_print_decreasing_valence(text2)

def plot_multiple_cumulative_scores(text_list, labels=None):
    """
    è¤‡æ•°ã®æ–‡ç« ãƒªã‚¹ãƒˆã«ã¤ã„ã¦ç´¯ç©æ‹æ„›æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã€
    1ã¤ã®ã‚°ãƒ©ãƒ•ã«é‡ã­ã¦ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ã€‚
    """
    plt.figure(figsize=(12, 6))

    if labels is None:
        labels = [f"ãƒ†ã‚­ã‚¹ãƒˆ{i+1}" for i in range(len(text_list))]

    for i, text in enumerate(text_list):
        _, cumulative_valence, _ = analyze_full_emotion_curve(text)
        x = list(range(1, len(cumulative_valence)+1))
        plt.plot(x, cumulative_valence, marker='o', label=labels[i])

    plt.title("è¤‡æ•°æ–‡ç« ã®ç´¯ç©æ‹æ„›æ„Ÿæƒ…ã‚¹ã‚³ã‚¢æ¨ç§»æ¯”è¼ƒ")
    plt.xlabel("æ–‡ç•ªå·")
    plt.ylabel("ç´¯ç©æ‹æ„›æ„Ÿæƒ…ã‚¹ã‚³ã‚¢")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

texts = [text1, text2]

# ãƒ©ãƒ™ãƒ«
labels = ["å‘Šç™½æˆç«‹", "æŒ¯ã‚‰ã‚Œã‚‹"]

# é–¢æ•°å‘¼ã³å‡ºã—
plot_multiple_cumulative_scores(texts, labels)

text3 = """å­¦æ ¡ã‚’å‡ºã‚‹ã¨ã€é¢¨ãŒä¹¾ã„ã¦ã„ãŸã€‚
é›²ãŒã„ãã¤ã‹æµ®ã‹ã³ã€é™½å°„ã—ã¯ã¾ã å¤ã®åæ®‹ã‚’å¼•ã„ã¦ã„ãŸã‘ã‚Œã©ã€é¢¨ã®ç«¯ã«ã¯å­£ç¯€ãŒæŠ˜ã‚Œã‚‹æ°—é…ãŒã‚ã£ãŸã€‚

é“ç«¯ã«å’²ããƒ ã‚¯ã‚²ã®èŠ±ãŒã€å°ã•ãæºã‚Œã¦ã„ã‚‹ã€‚
ã„ãã¤ã‹è½ã¡ãŸèŠ±ãŒã€åœ°é¢ã«è‰²ã®è·¡ã‚’æ®‹ã—ã¦ã„ãŸã€‚

ä¿¡å·ã®æ‰‹å‰ã§ã€å½¼å¥³ãŒå¾…ã£ã¦ã„ãŸã€‚

ã€Œâ€¦â€¦æ€ã£ãŸã‚ˆã‚Šã€æ—©ã‹ã£ãŸã­ã€

ã€Œãƒ—ãƒªãƒ³ãƒˆé…ã‚‰ã‚Œãªã‹ã£ãŸã€‚ã ã‹ã‚‰ã€

ãã‚Œã ã‘è¨€ã£ã¦ã€é„ã‚’è‚©ã«ã‹ã‘ç›´ã™ã€‚
é„ã®é‡‘å…·ãŒã‚ãšã‹ã«éŸ³ã‚’ç«‹ã¦ãŸã€‚æ­©ãå‡ºã™ã¾ã§ã€ãµãŸã‚Šã¯é»™ã£ã¦ã„ãŸã€‚

èˆ—è£…è·¯ã«ã¯ã€æ˜¨æ—¥ã®é›¨ã®ç—•ãŒã¾ã æ®‹ã£ã¦ã„ãŸã€‚
å ´æ‰€ã«ã‚ˆã£ã¦ã¯ä¹¾ã„ã¦ã„ãªã„ç®‡æ‰€ã‚‚ã‚ã‚‹ã€‚
å½¼ã¯ãã‚Œã‚’é¿ã‘ã‚‹ã§ã‚‚ãªãæ­©ãã€‚
æ°´é¢ãŒå…‰ã‚’è·³ã­è¿”ã—ã€è¶³éŸ³ãŒå¾®ã‹ã«å¤‰ã‚ã£ãŸã€‚

ãµãŸã‚Šã®æ­©å¹…ã¯ã€ã¨ãã©ããšã‚ŒãŸã€‚
æ­©é“ãŒç‹­ã„ã¨ã“ã‚ã§ã¯ã€è‡ªç„¶ã¨é †ç•ªãŒå…¥ã‚Œæ›¿ã‚ã£ãŸã€‚
ã©ã¡ã‚‰ã‚‚æ°—ã«ã™ã‚‹ãã¶ã‚Šã¯è¦‹ã›ãªã‹ã£ãŸã€‚

è‡ªè»¢è»Šã®ã‚¿ã‚¤ãƒ¤ãŒé€šã‚Šéãã€æ°´ãŒã‚¢ã‚¹ãƒ•ã‚¡ãƒ«ãƒˆã«è½ã¡ã‚‹ã€‚
å½¼ã¯ãã®éŸ³ã«ç›®ã‚’å‘ã‘ã€å½¼å¥³ã¯æ°—ã¥ã‹ãšå‰ã‚’è¦‹ã¦ã„ãŸã€‚

ã€Œä»Šæ—¥ã€é ãã§é›·é³´ã£ã¦ãŸã€

å½¼å¥³ãŒè¨€ã£ãŸã€‚

ã€Œé›·ï¼Ÿã€

ã€Œã†ã‚“ã€‚å¸°ã‚Šã®ç©ºã§ã€ã¡ã‚‡ã£ã¨ã€‚éŸ³ã ã‘ã ã‘ã©ã€

å½¼ã¯ã†ãªãšã„ãŸã‚ˆã†ã«è¦‹ãˆãŸãŒã€ã¯ã£ãã‚Šã¨ã¯ã—ãªã‹ã£ãŸã€‚
ä»£ã‚ã‚Šã«ã€ç©ºã‚’è¦‹ä¸Šã’ãŸã€‚é›²ã¯ã€ã•ã£ãã‚ˆã‚Šæµã‚Œã¦ã„ãŸã€‚

é›»æŸ±ã«ã€å°¾ã®é•·ã„åå‰ã‚’çŸ¥ã‚‰ãªã„é³¥ãŒã¨ã¾ã£ã¦ã„ãŸã€‚
ã‚«ãƒ©ã‚¹ã§ã¯ãªã‹ã£ãŸã€‚

ã©ã“ã‹ã®åº­å…ˆã‹ã‚‰ã€æ²ˆä¸èŠ±ã®ã‚ˆã†ãªé¦™ã‚ŠãŒæ¼‚ã£ã¦ããŸã€‚
æ˜¥ã§ã‚‚ç§‹ã§ã‚‚ãªã„ã€å­£ç¯€ã®ç«¯ã®åŒ‚ã„ã€‚

ã€Œå³ã€æ›²ãŒã‚‹ã‚“ã ã£ãŸã‚ˆã­ã€

å½¼å¥³ã®è¨€è‘‰ã«ã€å½¼ã¯å°‘ã—é…ã‚Œã¦ã†ãªãšã„ãŸã€‚

æ›²ãŒã‚Šè§’ã®å¡€ã«ã¯ã€è‹”ãŒè²¼ã‚Šã¤ã„ã¦ã„ãŸã€‚
ã²ã¨ã¤ã®è”¦ãŒã€å¡€ã®ä¸Šã‹ã‚‰å‚ã‚Œã¦é¢¨ã«æºã‚Œã¦ã„ãŸã€‚

ã‚‚ã†ã™ãã€åˆ†ã‹ã‚Œé“ãŒæ¥ã‚‹ã€‚

ã‘ã‚Œã©ã€ãã‚Œã‚’è¨€è‘‰ã«ã—ã‚ˆã†ã¨ã¯æ€ã‚ãªã‹ã£ãŸã€‚
è¨€è‘‰ã«ã™ã‚‹ã“ã¨ã®ã»ã†ãŒã€ä¸è‡ªç„¶ã«æ„Ÿã˜ãŸã€‚

ã€Œä»Šæœã•ã€é§…ã®ã¨ã“ã§çŒ«ãŒå¯ã¦ãŸã€

ã€ŒçŒ«ï¼Ÿã€

ã€Œã†ã‚“ã€‚çœ‹æ¿ã®è£ã«ã„ãŸã€‚èŒ¶è‰²ã„ã‚„ã¤ã€

å½¼å¥³ã¯ãã†è¨€ã£ã¦ã€å‰é«ªã‚’é¢¨ã‹ã‚‰ã‹ã°ã†ã‚ˆã†ã«æŒ‡å…ˆã§æŠ¼ã•ãˆãŸã€‚
æŒ‡ã®ã‹ãŸã¡ãŒã€ã»ã‚“ã®å°‘ã—æ®‹ã£ãŸã€‚

ã€Œãã®çŒ«ã€å‰ã«ã‚‚è¦‹ãŸæ°—ãŒã™ã‚‹ã€

ã€Œå‰ã£ã¦ã€ã„ã¤ï¼Ÿã€

ã€Œãšã£ã¨å‰ã€‚å¯’ããªã‚‹å‰ã€

é¢¨ãŒå°‘ã—å¼·ããªã‚Šã€è½ã¡è‘‰ãŒèˆã£ãŸã€‚
ãã®ã¨ãã€é›²ã®éš™é–“ã‹ã‚‰é™½å°„ã—ãŒã®ãã„ãŸã€‚
ãµãŸã‚Šã®å½±ãŒè¶³å…ƒã«å·®ã—ã€é•·ãä¸¦ã‚“ã§æºã‚Œã¦ã„ãŸã€‚

åˆ†ã‹ã‚Œé“ã¾ã§ã€ã‚ã¨åæ•°æ­©ã€‚
å½¼å¥³ã®é„ãŒå°‘ã—å‚¾ã„ã¦ã„ã‚‹ã®ã«æ°—ã¥ã„ãŸãŒã€å½¼ã¯ä½•ã‚‚è¨€ã‚ãªã‹ã£ãŸã€‚

è¶³ãŒæ­¢ã¾ã‚‹ã€‚

ã€Œã˜ã‚ƒã‚ã€ã¾ãŸã€

ã€Œã†ã‚“ã€

è¨€è‘‰ã¯ã€ãã‚Œã ã‘ã ã£ãŸã€‚
ã©ã¡ã‚‰ã‚‚æŒ¯ã‚Šè¿”ã‚‰ãªã‹ã£ãŸã€‚
å½±ãŒåˆ¥ã€…ã®æ–¹å‘ã¸ã¨ä¼¸ã³ã¦ã„ãã€‚

ãã®èƒŒä¸­ã«ã€ä½•ã‚‚ãªã‹ã£ãŸã‚ã‘ã§ã¯ãªã„ã€‚
"""

print_numbered_sentences(text3)

# --- å®Ÿè¡Œ ---
transitions, cumulative_valence, sentences = analyze_full_emotion_curve(text3)
plot_full_emotion_curve(transitions, cumulative_valence, sentences)

analyze_and_print_decreasing_valence(text3)

text4 = """è‡ªè»¢è»Šã‚’æŠ¼ã—ã¦å‚ã‚’ä¸‹ã‚‹ã¨ã€ç©ºæ°—ãŒã—ã£ã¨ã‚Šã—ã¦ã„ãŸã€‚
æœã«é™ã£ãŸé›¨ãŒã€ã¾ã ã‚¢ã‚¹ãƒ•ã‚¡ãƒ«ãƒˆã«åŒ‚ã„ã‚’æ®‹ã—ã¦ã„ã‚‹ã€‚é ãã§ã‚«ã‚¨ãƒ«ã®å£°ãŒã—ãŸã€‚

é“ã®ç«¯ã«ã€æ°´ãŸã¾ã‚ŠãŒã¨ã“ã‚ã©ã“ã‚æ®‹ã£ã¦ã„ãŸã€‚ãã“ã‚’é¿ã‘ã‚‹ã‚ˆã†ã«ã—ã¦æ­©ãã€‚
å‰ã‚’æ­©ã„ã¦ã„ã‚‹å½¼ã¯ã€æ°—ã«ã›ãšãã®ã¾ã¾é€²ã‚€ã€‚é´ã®è£ãŒã€æ°´ã‚’å¸ã£ã¦ã€å°‘ã—é»’ããªã£ã¦ã„ãŸã€‚

å£°ã‚’ã‹ã‘ã‚‹ã«ã¯è¿‘ã™ãã¦ã€ã‹ã‘ãªã„ã«ã¯é ã™ãã‚‹è·é›¢ã€‚
ãã‚Œã‚’ä¿ã¡ãªãŒã‚‰ã€ãµãŸã‚Šã¯ãšã£ã¨å‚ã‚’ä¸‹ã£ã¦ã„ãŸã€‚

ã€Œé›¨ã€ã¾ãŸé™ã‚Šãã†ã ã­ã€

å½¼ãŒè¨€ã£ãŸã€‚

ã€Œã†ã‚“ã€

çŸ­ãè¿”ã™ã€‚ã„ã¤ã‚‚ãã†ã€‚è¨€ã„ãŸã„ã“ã¨ã¯ã‚‚ã£ã¨ã‚ã‚‹ã®ã«ã€å‡ºã¦ãã‚‹ã®ã¯å˜èªã ã‘ã€‚

å‰é«ªãŒå°‘ã—æ¿¡ã‚Œã¦ã„ãŸã€‚ä¹¾ãã‹ã‘ã®ãã®éš™é–“ã‹ã‚‰ã€ç™½ã„é¦–ç­‹ãŒã¡ã‚‰ã‚Šã¨è¦‹ãˆãŸã€‚
ç›®ã‚’ãã‚‰ã—ãŸã€‚

é¢¨ãŒã€è‘‰ã‚’æºã‚‰ã—ã¦ã„ã‚‹ã€‚æ¿¡ã‚ŒãŸæœ¨ã®åŒ‚ã„ãŒã™ã‚‹ã€‚
ã“ã®å­£ç¯€ã®é¢¨ã¯ã€åå‰ã‚’ã¤ã‘ãŸããªã‚‹ã»ã©ã‚„ã•ã—ã„ã®ã«ã€ã‚ã¨ã«ä½•ã‚‚æ®‹ã•ãªã„ã€‚

å½¼ãŒãµã¨è¶³ã‚’æ­¢ã‚ãŸã€‚å°ã•ãªæ©‹ã®æ‰‹å‰ã§ã€‚

ã€Œè¦‹ã¦ã€‚å·ã€å¢—ãˆã¦ã‚‹ã€

å·ã®æ°´ãŒã€ã„ã¤ã‚‚ã‚ˆã‚Šé«˜ã„ä½ç½®ã¾ã§ãã¦ã„ãŸã€‚é›¨ã®ã›ã„ã ã‚ã†ã€‚
å½¼ã¯ã˜ã£ã¨è¦‹ã¦ã„ãŸã€‚æµã‚Œã®é€Ÿã•ã‚’ã€ç›®ã§è¿½ã£ã¦ã„ã‚‹ã€‚

ç§ã¯å½¼ã‚’è¦‹ã¦ã„ãŸã€‚å½¼ã®ç›®ç·šãŒã©ã“ã«å‘ã„ã¦ã„ã¦ã‚‚ã€ç§ã¯ãã‚Œã‚’ãªãã£ã¦ã„ã‚‹ã€‚

ä½•ã‚’è©±ã›ã°ã„ã„ã‹ã‚ã‹ã‚‰ãªã„ã¾ã¾ã€ã¾ãŸæ­©ãå‡ºã™ã€‚
å°ã•ãªéŸ³ã‚’ç«‹ã¦ã¦ã€è‡ªè»¢è»Šã®ãƒ–ãƒ¬ãƒ¼ã‚­ãŒé³´ã‚‹ã€‚ä½•ã‚‚è¨€ã‚ã‚Œãªã„ã‘ã©ã€åˆã‚ã›ã¦æ­©ãã®ãŒå°‘ã—ã ã‘ã†ã‚Œã—ã‹ã£ãŸã€‚

é€”ä¸­ã€å°ã•ãªè‰ãŒé“ã‚’ã¯ã¿å‡ºã—ã¦ä¼¸ã³ã¦ã„ãŸã€‚
èŠ±ãŒå’²ãã§ã‚‚ãªã„ã€åã‚‚ãªã„è‰ã€‚ã§ã‚‚ã€ã¡ã‚ƒã‚“ã¨ç”Ÿãã¦ã„ã‚‹ã€‚

ãã‚Œã‚’è¦‹ãªãŒã‚‰ã€æ€ã£ãŸã€‚ç§ã®æ°—æŒã¡ã‚‚ã€ã‚ã‚“ãªãµã†ã«ã©ã“ã‹ã«ä¼¸ã³ã¦ã„ã‘ãŸã‚‰ã„ã„ã€‚èª°ã«ã‚‚æ°—ã¥ã‹ã‚Œãªãã¦ã‚‚ã€‚

äº¤å·®ç‚¹ã®æ‰‹å‰ã§ã€å½¼ã¯å³ã¸ã€è‡ªåˆ†ã¯ã¾ã£ã™ãã€‚
ãã‚Œã¯ã„ã¤ã‚‚é€šã‚Šã§ã€å¤‰ãˆã‚‹ç†ç”±ã‚‚ãªã„ã€‚

ã€Œã˜ã‚ƒã‚ã­ã€

ã€Œã¾ãŸã€

èƒŒä¸­ã‚’è¦‹é€ã‚‹ã€‚ä¿¡å·ãŒå¤‰ã‚ã‚‹ã¾ã§ã€ãã®å§¿ã¯æ¶ˆãˆãªã‹ã£ãŸã€‚

è‡ªè»¢è»Šã®ãƒãƒ³ãƒ‰ãƒ«ã‚’æ¡ã‚Šç›´ã—ãŸã¨ãã€æ‰‹ã®ã²ã‚‰ãŒå°‘ã—æ¹¿ã£ã¦ã„ãŸã€‚
ãã‚ŒãŒé›¨ã®ã›ã„ãªã®ã‹ã€ãã‚Œã¨ã‚‚è‡ªåˆ†ã®ã›ã„ãªã®ã‹ã€ã‚ˆãã‚ã‹ã‚‰ãªã‹ã£ãŸã€‚"""

print_numbered_sentences(text4)

# --- å®Ÿè¡Œ ---
transitions, cumulative_valence, sentences = analyze_full_emotion_curve(text4)
plot_full_emotion_curve(transitions, cumulative_valence, sentences)

analyze_and_print_decreasing_valence(text4)